{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRS Spectrometer Neutrol network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import math\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Training dataset\n",
    "- Merge Multi dataset\n",
    "- randomize the order of the dataset\n",
    "- [ftp data server](http://localhost/Data/spectro_nn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file ../dataGenerator/result/PRex_DataSet_Full_2239.csv\n",
      "Loading file ../dataGenerator/result/PRex_DataSet_Full_2240.csv\n",
      "Loading file ../dataGenerator/result/PRex_DataSet_Full_2241.csv\n",
      "Loading file ../dataGenerator/result/PRex_DataSet_Full_2244.csv\n",
      "Loading file ../dataGenerator/result/PRex_DataSet_Full_2245.csv\n",
      "Loading file ../dataGenerator/result/PRex_DataSet_Full_2256.csv\n",
      "Loading file ../dataGenerator/result/PRex_DataSet_Full_2257.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "   evtID  runID  CutID  SieveRowID  SieveColID      bpmX      bpmY  \\\n0      0   2239    136           3           5  0.003794 -0.000501   \n1      1   2239    130           4           4  0.003794 -0.000501   \n2      2   2239    179           4          11  0.003794 -0.000501   \n3      3   2239    142           2           6  0.003794 -0.000501   \n4      4   2239    127           1           4  0.003794 -0.000501   \n\n   x0th0y0ph0  x0th0y0ph1  x0th0y0ph2  ...    x0th1y0ph1  x0th1y1ph0  \\\n0           1   -0.001618    0.000003  ... -8.923140e-07    0.000006   \n1           1   -0.009000    0.000081  ...  6.038210e-05   -0.000057   \n2           1    0.014969    0.000224  ... -9.636200e-05    0.000094   \n3           1    0.001385    0.000002  ...  7.749150e-06    0.000017   \n4           1   -0.004135    0.000017  ... -2.666770e-05    0.000078   \n\n     x0th2y0ph0  x1th0y0ph0  x1th0y0ph1  x1th0y1ph0  x1th1y0ph0  x2th0y0ph0  \\\n0  3.042260e-07   -0.014535    0.000024   -0.000161   -0.000008    0.000211   \n1  4.501310e-05   -0.027425    0.000247   -0.000233    0.000184    0.000752   \n2  4.144110e-05   -0.013932   -0.000209    0.000204    0.000090    0.000194   \n3  3.129790e-05   -0.010740   -0.000015   -0.000032   -0.000060    0.000115   \n4  4.159330e-05   -0.002179    0.000009   -0.000026   -0.000014    0.000005   \n\n   targCalTh  targCalPh  \n0  -0.000817  -0.010117  \n1   0.019571  -0.012569  \n2   0.012775   0.014835  \n3  -0.014409  -0.003863  \n4  -0.021206  -0.012569  \n\n[5 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>evtID</th>\n      <th>runID</th>\n      <th>CutID</th>\n      <th>SieveRowID</th>\n      <th>SieveColID</th>\n      <th>bpmX</th>\n      <th>bpmY</th>\n      <th>x0th0y0ph0</th>\n      <th>x0th0y0ph1</th>\n      <th>x0th0y0ph2</th>\n      <th>...</th>\n      <th>x0th1y0ph1</th>\n      <th>x0th1y1ph0</th>\n      <th>x0th2y0ph0</th>\n      <th>x1th0y0ph0</th>\n      <th>x1th0y0ph1</th>\n      <th>x1th0y1ph0</th>\n      <th>x1th1y0ph0</th>\n      <th>x2th0y0ph0</th>\n      <th>targCalTh</th>\n      <th>targCalPh</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2239</td>\n      <td>136</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0.003794</td>\n      <td>-0.000501</td>\n      <td>1</td>\n      <td>-0.001618</td>\n      <td>0.000003</td>\n      <td>...</td>\n      <td>-8.923140e-07</td>\n      <td>0.000006</td>\n      <td>3.042260e-07</td>\n      <td>-0.014535</td>\n      <td>0.000024</td>\n      <td>-0.000161</td>\n      <td>-0.000008</td>\n      <td>0.000211</td>\n      <td>-0.000817</td>\n      <td>-0.010117</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2239</td>\n      <td>130</td>\n      <td>4</td>\n      <td>4</td>\n      <td>0.003794</td>\n      <td>-0.000501</td>\n      <td>1</td>\n      <td>-0.009000</td>\n      <td>0.000081</td>\n      <td>...</td>\n      <td>6.038210e-05</td>\n      <td>-0.000057</td>\n      <td>4.501310e-05</td>\n      <td>-0.027425</td>\n      <td>0.000247</td>\n      <td>-0.000233</td>\n      <td>0.000184</td>\n      <td>0.000752</td>\n      <td>0.019571</td>\n      <td>-0.012569</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2239</td>\n      <td>179</td>\n      <td>4</td>\n      <td>11</td>\n      <td>0.003794</td>\n      <td>-0.000501</td>\n      <td>1</td>\n      <td>0.014969</td>\n      <td>0.000224</td>\n      <td>...</td>\n      <td>-9.636200e-05</td>\n      <td>0.000094</td>\n      <td>4.144110e-05</td>\n      <td>-0.013932</td>\n      <td>-0.000209</td>\n      <td>0.000204</td>\n      <td>0.000090</td>\n      <td>0.000194</td>\n      <td>0.012775</td>\n      <td>0.014835</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2239</td>\n      <td>142</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.003794</td>\n      <td>-0.000501</td>\n      <td>1</td>\n      <td>0.001385</td>\n      <td>0.000002</td>\n      <td>...</td>\n      <td>7.749150e-06</td>\n      <td>0.000017</td>\n      <td>3.129790e-05</td>\n      <td>-0.010740</td>\n      <td>-0.000015</td>\n      <td>-0.000032</td>\n      <td>-0.000060</td>\n      <td>0.000115</td>\n      <td>-0.014409</td>\n      <td>-0.003863</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2239</td>\n      <td>127</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.003794</td>\n      <td>-0.000501</td>\n      <td>1</td>\n      <td>-0.004135</td>\n      <td>0.000017</td>\n      <td>...</td>\n      <td>-2.666770e-05</td>\n      <td>0.000078</td>\n      <td>4.159330e-05</td>\n      <td>-0.002179</td>\n      <td>0.000009</td>\n      <td>-0.000026</td>\n      <td>-0.000014</td>\n      <td>0.000005</td>\n      <td>-0.021206</td>\n      <td>-0.012569</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 24 columns</p>\n</div>"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files = [\"../dataGenerator/result/PRex_DataSet_2239.csv\",\n",
    "#          \"../dataGenerator/result/PRex_DataSet_2240.csv\",\n",
    "#          \"../dataGenerator/result/PRex_DataSet_2241.csv\",\n",
    "#          \"../dataGenerator/result/PRex_DataSet_2244.csv\",\n",
    "#          \"../dataGenerator/result/PRex_DataSet_2245.csv\",\n",
    "#          \"../dataGenerator/result/PRex_DataSet_2256.csv\",\n",
    "#          \"../dataGenerator/result/PRex_DataSet_2257.csv\"\n",
    "#          ]\n",
    "\n",
    "# large data set with all the features \n",
    "files = [\"../dataGenerator/result/PRex_DataSet_Full_2239.csv\",\n",
    "         \"../dataGenerator/result/PRex_DataSet_Full_2240.csv\",\n",
    "         \"../dataGenerator/result/PRex_DataSet_Full_2241.csv\",\n",
    "         \"../dataGenerator/result/PRex_DataSet_Full_2244.csv\",\n",
    "         \"../dataGenerator/result/PRex_DataSet_Full_2245.csv\",\n",
    "         \"../dataGenerator/result/PRex_DataSet_Full_2256.csv\",\n",
    "         \"../dataGenerator/result/PRex_DataSet_Full_2257.csv\"\n",
    "         ]\n",
    "li = []\n",
    "for filename in files:\n",
    "    print(\"Loading file {}\".format(filename))\n",
    "    df = pd.read_csv(filename)\n",
    "    li.append(df)\n",
    "data = pd.concat(li)\n",
    "data.sample(frac=1) # sample the data with fraction 1. rearrange the data in random order\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(data[data.runID.eq(2240)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "x = data[data.runID.eq(2241)][\"targCalPh\"]\n",
    "y = data[data.runID.eq(2241)][\"targCalTh\"]\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Function Used for Train the X, and Y model seperately"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "data": {
      "text/plain": "(221493, 24)"
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics = ['int16','int32','int64','float16','float32','float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape\n",
    "\n",
    "### 1. prepare the training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "((177194, 15), (177194, 2), (44299, 15), (44299, 2))"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    data.drop(labels=['evtID','runID','SieveRowID','SieveColID','CutID','bpmX','bpmY','targCalTh','targCalPh'], axis=1),\n",
    "    data[['targCalTh','targCalPh']],\n",
    "    test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "n_feature = train_X.shape[1]\n",
    "train_X.shape,train_y.shape,test_X.shape,test_y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OptNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OptNet,self).__init__()\n",
    "        #TODO model, need to auto adapt to the training dataset\n",
    "        self.fc1 = nn.Linear(n_feature, 2*n_feature)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(2*n_feature, n_feature)\n",
    "        self.fc3 = nn.Linear(n_feature, n_feature//2)\n",
    "        self.output = nn.Linear(n_feature//2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training on the $\\theta_{targ}$\n",
    "\n",
    "### 1. train the $\\theta_{targ}$ model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_trainTh = torch.tensor(train_X.to_numpy()).float()\n",
    "y_trainTh = torch.tensor(train_y['targCalTh'].to_numpy()).float()\n",
    "X_testTh  = torch.tensor(test_X.to_numpy()).float()\n",
    "y_testTh  = torch.tensor(test_y['targCalTh'].to_numpy()).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000, 0.010000] loss: 0.000605943\n",
      "[1,  4000, 0.010000] loss: 0.000146011\n",
      "[1,  6000, 0.010000] loss: 0.000035042\n",
      "[1,  8000, 0.010000] loss: 0.000038661\n",
      "[1, 10000, 0.010000] loss: 0.000035084\n",
      "[1, 12000, 0.010000] loss: 0.000034806\n",
      "[1, 14000, 0.010000] loss: 0.000036102\n",
      "[1, 16000, 0.010000] loss: 0.000032646\n",
      "[1, 18000, 0.010000] loss: 0.000035122\n",
      "[1, 20000, 0.010000] loss: 0.000029902\n",
      "[1, 22000, 0.010000] loss: 0.000031688\n",
      "[1, 24000, 0.010000] loss: 0.000029638\n",
      "[1, 26000, 0.010000] loss: 0.000030325\n",
      "[1, 28000, 0.010000] loss: 0.000029086\n",
      "[1, 30000, 0.010000] loss: 0.000029976\n",
      "[1, 32000, 0.010000] loss: 0.000028646\n",
      "[1, 34000, 0.010000] loss: 0.000024608\n",
      "[1, 36000, 0.010000] loss: 0.000021403\n",
      "[1, 38000, 0.010000] loss: 0.000022113\n",
      "[1, 40000, 0.010000] loss: 0.000021777\n",
      "[1, 42000, 0.010000] loss: 0.000026157\n",
      "[1, 44000, 0.010000] loss: 0.000021145\n",
      "[1, 46000, 0.010000] loss: 0.000020079\n",
      "[1, 48000, 0.010000] loss: 0.000021941\n",
      "[1, 50000, 0.010000] loss: 0.000020775\n",
      "[1, 52000, 0.010000] loss: 0.000024079\n",
      "[1, 54000, 0.010000] loss: 0.000020202\n",
      "[1, 56000, 0.010000] loss: 0.000020647\n",
      "[1, 58000, 0.010000] loss: 0.000019636\n",
      "[1, 60000, 0.010000] loss: 0.000021134\n",
      "[1, 62000, 0.010000] loss: 0.000019545\n",
      "[1, 64000, 0.010000] loss: 0.000018258\n",
      "[1, 66000, 0.010000] loss: 0.000020188\n",
      "[1, 68000, 0.010000] loss: 0.000019482\n",
      "[1, 70000, 0.010000] loss: 0.000020592\n",
      "[1, 72000, 0.010000] loss: 0.000019193\n",
      "[1, 74000, 0.010000] loss: 0.000017710\n",
      "[1, 76000, 0.010000] loss: 0.000018507\n",
      "[1, 78000, 0.010000] loss: 0.000019655\n",
      "[1, 80000, 0.010000] loss: 0.000018507\n",
      "[1, 82000, 0.010000] loss: 0.000018886\n",
      "[1, 84000, 0.010000] loss: 0.000025530\n",
      "[1, 86000, 0.010000] loss: 0.000016990\n",
      "[1, 88000, 0.010000] loss: 0.000018436\n",
      "[1, 90000, 0.010000] loss: 0.000018264\n",
      "[1, 92000, 0.010000] loss: 0.000018207\n",
      "[1, 94000, 0.010000] loss: 0.000018283\n",
      "[1, 96000, 0.010000] loss: 0.000018273\n",
      "[1, 98000, 0.010000] loss: 0.000018420\n",
      "[1, 100000, 0.010000] loss: 0.000019038\n",
      "[1, 102000, 0.010000] loss: 0.000018015\n",
      "[1, 104000, 0.010000] loss: 0.000014334\n",
      "[1, 106000, 0.010000] loss: 0.000022067\n",
      "[1, 108000, 0.010000] loss: 0.000015820\n",
      "[1, 110000, 0.010000] loss: 0.000019153\n",
      "[1, 112000, 0.010000] loss: 0.000015935\n",
      "[1, 114000, 0.010000] loss: 0.000016156\n",
      "[1, 116000, 0.010000] loss: 0.000015010\n",
      "[1, 118000, 0.010000] loss: 0.000016094\n",
      "[1, 120000, 0.010000] loss: 0.000018043\n",
      "[1, 122000, 0.010000] loss: 0.000015632\n",
      "[1, 124000, 0.010000] loss: 0.000018755\n",
      "[1, 126000, 0.010000] loss: 0.000014289\n",
      "[1, 128000, 0.010000] loss: 0.000015975\n",
      "[1, 130000, 0.010000] loss: 0.000016278\n",
      "[1, 132000, 0.010000] loss: 0.000017174\n",
      "[1, 134000, 0.010000] loss: 0.000018700\n",
      "[1, 136000, 0.010000] loss: 0.000013436\n",
      "[1, 138000, 0.010000] loss: 0.000013477\n",
      "[1, 140000, 0.010000] loss: 0.000018939\n",
      "[1, 142000, 0.010000] loss: 0.000016608\n",
      "[1, 144000, 0.010000] loss: 0.000017009\n",
      "[1, 146000, 0.010000] loss: 0.000016738\n",
      "[1, 148000, 0.010000] loss: 0.000013652\n",
      "[1, 150000, 0.010000] loss: 0.000015668\n",
      "[1, 152000, 0.010000] loss: 0.000017545\n",
      "[1, 154000, 0.010000] loss: 0.000015032\n",
      "[1, 156000, 0.010000] loss: 0.000016748\n",
      "[1, 158000, 0.010000] loss: 0.000015402\n",
      "[1, 160000, 0.010000] loss: 0.000015942\n",
      "[1, 162000, 0.010000] loss: 0.000015214\n",
      "[1, 164000, 0.010000] loss: 0.000015550\n",
      "[1, 166000, 0.010000] loss: 0.000014836\n",
      "[1, 168000, 0.010000] loss: 0.000016345\n",
      "[1, 170000, 0.010000] loss: 0.000013534\n",
      "[1, 172000, 0.010000] loss: 0.000017196\n",
      "[1, 174000, 0.010000] loss: 0.000013677\n",
      "[1, 176000, 0.010000] loss: 0.000016723\n",
      "[2,  2000, 0.001000] loss: 0.000005669\n",
      "[2,  4000, 0.001000] loss: 0.000004811\n",
      "[2,  6000, 0.001000] loss: 0.000005421\n",
      "[2,  8000, 0.001000] loss: 0.000005721\n",
      "[2, 10000, 0.001000] loss: 0.000005225\n",
      "[2, 12000, 0.001000] loss: 0.000005278\n",
      "[2, 14000, 0.001000] loss: 0.000005362\n",
      "[2, 16000, 0.001000] loss: 0.000005335\n",
      "[2, 18000, 0.001000] loss: 0.000005741\n",
      "[2, 20000, 0.001000] loss: 0.000005343\n",
      "[2, 22000, 0.001000] loss: 0.000005644\n",
      "[2, 24000, 0.001000] loss: 0.000005167\n",
      "[2, 26000, 0.001000] loss: 0.000005479\n",
      "[2, 28000, 0.001000] loss: 0.000005346\n",
      "[2, 30000, 0.001000] loss: 0.000005416\n",
      "[2, 32000, 0.001000] loss: 0.000005286\n",
      "[2, 34000, 0.001000] loss: 0.000005171\n",
      "[2, 36000, 0.001000] loss: 0.000005691\n",
      "[2, 38000, 0.001000] loss: 0.000005201\n",
      "[2, 40000, 0.001000] loss: 0.000005704\n",
      "[2, 42000, 0.001000] loss: 0.000005145\n",
      "[2, 44000, 0.001000] loss: 0.000005434\n",
      "[2, 46000, 0.001000] loss: 0.000004965\n",
      "[2, 48000, 0.001000] loss: 0.000005607\n",
      "[2, 50000, 0.001000] loss: 0.000004970\n",
      "[2, 52000, 0.001000] loss: 0.000005117\n",
      "[2, 54000, 0.001000] loss: 0.000005211\n",
      "[2, 56000, 0.001000] loss: 0.000005279\n",
      "[2, 58000, 0.001000] loss: 0.000005293\n",
      "[2, 60000, 0.001000] loss: 0.000005840\n",
      "[2, 62000, 0.001000] loss: 0.000004797\n",
      "[2, 64000, 0.001000] loss: 0.000005103\n",
      "[2, 66000, 0.001000] loss: 0.000005222\n",
      "[2, 68000, 0.001000] loss: 0.000005362\n",
      "[2, 70000, 0.001000] loss: 0.000005166\n",
      "[2, 72000, 0.001000] loss: 0.000005417\n",
      "[2, 74000, 0.001000] loss: 0.000005044\n",
      "[2, 76000, 0.001000] loss: 0.000005698\n",
      "[2, 78000, 0.001000] loss: 0.000005235\n",
      "[2, 80000, 0.001000] loss: 0.000005109\n",
      "[2, 82000, 0.001000] loss: 0.000005420\n",
      "[2, 84000, 0.001000] loss: 0.000005787\n",
      "[2, 86000, 0.001000] loss: 0.000004873\n",
      "[2, 88000, 0.001000] loss: 0.000005110\n",
      "[2, 90000, 0.001000] loss: 0.000005177\n",
      "[2, 92000, 0.001000] loss: 0.000005247\n",
      "[2, 94000, 0.001000] loss: 0.000005181\n",
      "[2, 96000, 0.001000] loss: 0.000005183\n",
      "[2, 98000, 0.001000] loss: 0.000005747\n",
      "[2, 100000, 0.001000] loss: 0.000005058\n",
      "[2, 102000, 0.001000] loss: 0.000005254\n",
      "[2, 104000, 0.001000] loss: 0.000004830\n",
      "[2, 106000, 0.001000] loss: 0.000005320\n",
      "[2, 108000, 0.001000] loss: 0.000005332\n",
      "[2, 110000, 0.001000] loss: 0.000005441\n",
      "[2, 112000, 0.001000] loss: 0.000004943\n",
      "[2, 114000, 0.001000] loss: 0.000005119\n",
      "[2, 116000, 0.001000] loss: 0.000004688\n",
      "[2, 118000, 0.001000] loss: 0.000005102\n",
      "[2, 120000, 0.001000] loss: 0.000005380\n",
      "[2, 122000, 0.001000] loss: 0.000004919\n",
      "[2, 124000, 0.001000] loss: 0.000005214\n",
      "[2, 126000, 0.001000] loss: 0.000005166\n",
      "[2, 128000, 0.001000] loss: 0.000005108\n",
      "[2, 130000, 0.001000] loss: 0.000005151\n",
      "[2, 132000, 0.001000] loss: 0.000005151\n",
      "[2, 134000, 0.001000] loss: 0.000005376\n",
      "[2, 136000, 0.001000] loss: 0.000004834\n",
      "[2, 138000, 0.001000] loss: 0.000005068\n",
      "[2, 140000, 0.001000] loss: 0.000005106\n",
      "[2, 142000, 0.001000] loss: 0.000004875\n",
      "[2, 144000, 0.001000] loss: 0.000005606\n",
      "[2, 146000, 0.001000] loss: 0.000004873\n",
      "[2, 148000, 0.001000] loss: 0.000005203\n",
      "[2, 150000, 0.001000] loss: 0.000005184\n",
      "[2, 152000, 0.001000] loss: 0.000005392\n",
      "[2, 154000, 0.001000] loss: 0.000005737\n",
      "[2, 156000, 0.001000] loss: 0.000005409\n",
      "[2, 158000, 0.001000] loss: 0.000004864\n",
      "[2, 160000, 0.001000] loss: 0.000004917\n",
      "[2, 162000, 0.001000] loss: 0.000005281\n",
      "[2, 164000, 0.001000] loss: 0.000004864\n",
      "[2, 166000, 0.001000] loss: 0.000004888\n",
      "[2, 168000, 0.001000] loss: 0.000004853\n",
      "[2, 170000, 0.001000] loss: 0.000005164\n",
      "[2, 172000, 0.001000] loss: 0.000005338\n",
      "[2, 174000, 0.001000] loss: 0.000005147\n",
      "[2, 176000, 0.001000] loss: 0.000005200\n",
      "[3,  2000, 0.000100] loss: 0.000004648\n",
      "[3,  4000, 0.000100] loss: 0.000003758\n",
      "[3,  6000, 0.000100] loss: 0.000004233\n",
      "[3,  8000, 0.000100] loss: 0.000004382\n",
      "[3, 10000, 0.000100] loss: 0.000004121\n",
      "[3, 12000, 0.000100] loss: 0.000004088\n",
      "[3, 14000, 0.000100] loss: 0.000004178\n",
      "[3, 16000, 0.000100] loss: 0.000004162\n",
      "[3, 18000, 0.000100] loss: 0.000004507\n",
      "[3, 20000, 0.000100] loss: 0.000004109\n",
      "[3, 22000, 0.000100] loss: 0.000004245\n",
      "[3, 24000, 0.000100] loss: 0.000004074\n",
      "[3, 26000, 0.000100] loss: 0.000004278\n",
      "[3, 28000, 0.000100] loss: 0.000004153\n",
      "[3, 30000, 0.000100] loss: 0.000004298\n",
      "[3, 32000, 0.000100] loss: 0.000004032\n",
      "[3, 34000, 0.000100] loss: 0.000004179\n",
      "[3, 36000, 0.000100] loss: 0.000004278\n",
      "[3, 38000, 0.000100] loss: 0.000004043\n",
      "[3, 40000, 0.000100] loss: 0.000004350\n",
      "[3, 42000, 0.000100] loss: 0.000004009\n",
      "[3, 44000, 0.000100] loss: 0.000004304\n",
      "[3, 46000, 0.000100] loss: 0.000003915\n",
      "[3, 48000, 0.000100] loss: 0.000004226\n",
      "[3, 50000, 0.000100] loss: 0.000003963\n",
      "[3, 52000, 0.000100] loss: 0.000003988\n",
      "[3, 54000, 0.000100] loss: 0.000004096\n",
      "[3, 56000, 0.000100] loss: 0.000004144\n",
      "[3, 58000, 0.000100] loss: 0.000003930\n",
      "[3, 60000, 0.000100] loss: 0.000004463\n",
      "[3, 62000, 0.000100] loss: 0.000003836\n",
      "[3, 64000, 0.000100] loss: 0.000003993\n",
      "[3, 66000, 0.000100] loss: 0.000004013\n",
      "[3, 68000, 0.000100] loss: 0.000004178\n",
      "[3, 70000, 0.000100] loss: 0.000004095\n",
      "[3, 72000, 0.000100] loss: 0.000004195\n",
      "[3, 74000, 0.000100] loss: 0.000004029\n",
      "[3, 76000, 0.000100] loss: 0.000004396\n",
      "[3, 78000, 0.000100] loss: 0.000004132\n",
      "[3, 80000, 0.000100] loss: 0.000003941\n",
      "[3, 82000, 0.000100] loss: 0.000004215\n",
      "[3, 84000, 0.000100] loss: 0.000004488\n",
      "[3, 86000, 0.000100] loss: 0.000003993\n",
      "[3, 88000, 0.000100] loss: 0.000003913\n",
      "[3, 90000, 0.000100] loss: 0.000004060\n",
      "[3, 92000, 0.000100] loss: 0.000003949\n",
      "[3, 94000, 0.000100] loss: 0.000004200\n",
      "[3, 96000, 0.000100] loss: 0.000004107\n",
      "[3, 98000, 0.000100] loss: 0.000004411\n",
      "[3, 100000, 0.000100] loss: 0.000004051\n",
      "[3, 102000, 0.000100] loss: 0.000004201\n",
      "[3, 104000, 0.000100] loss: 0.000003720\n",
      "[3, 106000, 0.000100] loss: 0.000004235\n",
      "[3, 108000, 0.000100] loss: 0.000004073\n",
      "[3, 110000, 0.000100] loss: 0.000004208\n",
      "[3, 112000, 0.000100] loss: 0.000003850\n",
      "[3, 114000, 0.000100] loss: 0.000003982\n",
      "[3, 116000, 0.000100] loss: 0.000003833\n",
      "[3, 118000, 0.000100] loss: 0.000003987\n",
      "[3, 120000, 0.000100] loss: 0.000004277\n",
      "[3, 122000, 0.000100] loss: 0.000003808\n",
      "[3, 124000, 0.000100] loss: 0.000004122\n",
      "[3, 126000, 0.000100] loss: 0.000004029\n",
      "[3, 128000, 0.000100] loss: 0.000003985\n",
      "[3, 130000, 0.000100] loss: 0.000004018\n",
      "[3, 132000, 0.000100] loss: 0.000004067\n",
      "[3, 134000, 0.000100] loss: 0.000004126\n",
      "[3, 136000, 0.000100] loss: 0.000003915\n",
      "[3, 138000, 0.000100] loss: 0.000003965\n",
      "[3, 140000, 0.000100] loss: 0.000004057\n",
      "[3, 142000, 0.000100] loss: 0.000003942\n",
      "[3, 144000, 0.000100] loss: 0.000004240\n",
      "[3, 146000, 0.000100] loss: 0.000003984\n",
      "[3, 148000, 0.000100] loss: 0.000004004\n",
      "[3, 150000, 0.000100] loss: 0.000004014\n",
      "[3, 152000, 0.000100] loss: 0.000004225\n",
      "[3, 154000, 0.000100] loss: 0.000004306\n",
      "[3, 156000, 0.000100] loss: 0.000004299\n",
      "[3, 158000, 0.000100] loss: 0.000003940\n",
      "[3, 160000, 0.000100] loss: 0.000003826\n",
      "[3, 162000, 0.000100] loss: 0.000004128\n",
      "[3, 164000, 0.000100] loss: 0.000003816\n",
      "[3, 166000, 0.000100] loss: 0.000003924\n",
      "[3, 168000, 0.000100] loss: 0.000003924\n",
      "[3, 170000, 0.000100] loss: 0.000003828\n",
      "[3, 172000, 0.000100] loss: 0.000004155\n",
      "[3, 174000, 0.000100] loss: 0.000004019\n",
      "[3, 176000, 0.000100] loss: 0.000004049\n",
      "[4,  2000, 0.000010] loss: 0.000004472\n",
      "[4,  4000, 0.000010] loss: 0.000003690\n",
      "[4,  6000, 0.000010] loss: 0.000004143\n",
      "[4,  8000, 0.000010] loss: 0.000004247\n",
      "[4, 10000, 0.000010] loss: 0.000003992\n",
      "[4, 12000, 0.000010] loss: 0.000003939\n",
      "[4, 14000, 0.000010] loss: 0.000004049\n",
      "[4, 16000, 0.000010] loss: 0.000004028\n",
      "[4, 18000, 0.000010] loss: 0.000004347\n",
      "[4, 20000, 0.000010] loss: 0.000003986\n",
      "[4, 22000, 0.000010] loss: 0.000004126\n",
      "[4, 24000, 0.000010] loss: 0.000003977\n",
      "[4, 26000, 0.000010] loss: 0.000004185\n",
      "[4, 28000, 0.000010] loss: 0.000004028\n",
      "[4, 30000, 0.000010] loss: 0.000004156\n",
      "[4, 32000, 0.000010] loss: 0.000003910\n",
      "[4, 34000, 0.000010] loss: 0.000004034\n",
      "[4, 36000, 0.000010] loss: 0.000004124\n",
      "[4, 38000, 0.000010] loss: 0.000003901\n",
      "[4, 40000, 0.000010] loss: 0.000004236\n",
      "[4, 42000, 0.000010] loss: 0.000003886\n",
      "[4, 44000, 0.000010] loss: 0.000004168\n",
      "[4, 46000, 0.000010] loss: 0.000003791\n",
      "[4, 48000, 0.000010] loss: 0.000004114\n",
      "[4, 50000, 0.000010] loss: 0.000003841\n",
      "[4, 52000, 0.000010] loss: 0.000003887\n",
      "[4, 54000, 0.000010] loss: 0.000004003\n",
      "[4, 56000, 0.000010] loss: 0.000004038\n",
      "[4, 58000, 0.000010] loss: 0.000003834\n",
      "[4, 60000, 0.000010] loss: 0.000004357\n",
      "[4, 62000, 0.000010] loss: 0.000003740\n",
      "[4, 64000, 0.000010] loss: 0.000003873\n",
      "[4, 66000, 0.000010] loss: 0.000003873\n",
      "[4, 68000, 0.000010] loss: 0.000004067\n",
      "[4, 70000, 0.000010] loss: 0.000003969\n",
      "[4, 72000, 0.000010] loss: 0.000004076\n",
      "[4, 74000, 0.000010] loss: 0.000003886\n",
      "[4, 76000, 0.000010] loss: 0.000004247\n",
      "[4, 78000, 0.000010] loss: 0.000004048\n",
      "[4, 80000, 0.000010] loss: 0.000003834\n",
      "[4, 82000, 0.000010] loss: 0.000004083\n",
      "[4, 84000, 0.000010] loss: 0.000004352\n",
      "[4, 86000, 0.000010] loss: 0.000003851\n",
      "[4, 88000, 0.000010] loss: 0.000003802\n",
      "[4, 90000, 0.000010] loss: 0.000003948\n",
      "[4, 92000, 0.000010] loss: 0.000003836\n",
      "[4, 94000, 0.000010] loss: 0.000004078\n",
      "[4, 96000, 0.000010] loss: 0.000004011\n",
      "[4, 98000, 0.000010] loss: 0.000004264\n",
      "[4, 100000, 0.000010] loss: 0.000003934\n",
      "[4, 102000, 0.000010] loss: 0.000004084\n",
      "[4, 104000, 0.000010] loss: 0.000003587\n",
      "[4, 106000, 0.000010] loss: 0.000004120\n",
      "[4, 108000, 0.000010] loss: 0.000003954\n",
      "[4, 110000, 0.000010] loss: 0.000004092\n",
      "[4, 112000, 0.000010] loss: 0.000003739\n",
      "[4, 114000, 0.000010] loss: 0.000003853\n",
      "[4, 116000, 0.000010] loss: 0.000003719\n",
      "[4, 118000, 0.000010] loss: 0.000003863\n",
      "[4, 120000, 0.000010] loss: 0.000004159\n",
      "[4, 122000, 0.000010] loss: 0.000003688\n",
      "[4, 124000, 0.000010] loss: 0.000003979\n",
      "[4, 126000, 0.000010] loss: 0.000003897\n",
      "[4, 128000, 0.000010] loss: 0.000003873\n",
      "[4, 130000, 0.000010] loss: 0.000003905\n",
      "[4, 132000, 0.000010] loss: 0.000003940\n",
      "[4, 134000, 0.000010] loss: 0.000003963\n",
      "[4, 136000, 0.000010] loss: 0.000003787\n",
      "[4, 138000, 0.000010] loss: 0.000003881\n",
      "[4, 140000, 0.000010] loss: 0.000003910\n",
      "[4, 142000, 0.000010] loss: 0.000003840\n",
      "[4, 144000, 0.000010] loss: 0.000004090\n",
      "[4, 146000, 0.000010] loss: 0.000003896\n",
      "[4, 148000, 0.000010] loss: 0.000003893\n",
      "[4, 150000, 0.000010] loss: 0.000003900\n",
      "[4, 152000, 0.000010] loss: 0.000004094\n",
      "[4, 154000, 0.000010] loss: 0.000004196\n",
      "[4, 156000, 0.000010] loss: 0.000004157\n",
      "[4, 158000, 0.000010] loss: 0.000003834\n",
      "[4, 160000, 0.000010] loss: 0.000003733\n",
      "[4, 162000, 0.000010] loss: 0.000004002\n",
      "[4, 164000, 0.000010] loss: 0.000003685\n",
      "[4, 166000, 0.000010] loss: 0.000003809\n",
      "[4, 168000, 0.000010] loss: 0.000003815\n",
      "[4, 170000, 0.000010] loss: 0.000003693\n",
      "[4, 172000, 0.000010] loss: 0.000004053\n",
      "[4, 174000, 0.000010] loss: 0.000003923\n",
      "[4, 176000, 0.000010] loss: 0.000003968\n",
      "[5,  2000, 0.000001] loss: 0.000004460\n",
      "[5,  4000, 0.000001] loss: 0.000003668\n",
      "[5,  6000, 0.000001] loss: 0.000004119\n",
      "[5,  8000, 0.000001] loss: 0.000004243\n",
      "[5, 10000, 0.000001] loss: 0.000003968\n",
      "[5, 12000, 0.000001] loss: 0.000003928\n",
      "[5, 14000, 0.000001] loss: 0.000004032\n",
      "[5, 16000, 0.000001] loss: 0.000004010\n",
      "[5, 18000, 0.000001] loss: 0.000004330\n",
      "[5, 20000, 0.000001] loss: 0.000003977\n",
      "[5, 22000, 0.000001] loss: 0.000004113\n",
      "[5, 24000, 0.000001] loss: 0.000003970\n",
      "[5, 26000, 0.000001] loss: 0.000004174\n",
      "[5, 28000, 0.000001] loss: 0.000004010\n",
      "[5, 30000, 0.000001] loss: 0.000004138\n",
      "[5, 32000, 0.000001] loss: 0.000003898\n",
      "[5, 34000, 0.000001] loss: 0.000004018\n",
      "[5, 36000, 0.000001] loss: 0.000004111\n",
      "[5, 38000, 0.000001] loss: 0.000003892\n",
      "[5, 40000, 0.000001] loss: 0.000004222\n",
      "[5, 42000, 0.000001] loss: 0.000003871\n",
      "[5, 44000, 0.000001] loss: 0.000004160\n",
      "[5, 46000, 0.000001] loss: 0.000003779\n",
      "[5, 48000, 0.000001] loss: 0.000004105\n",
      "[5, 50000, 0.000001] loss: 0.000003827\n",
      "[5, 52000, 0.000001] loss: 0.000003869\n",
      "[5, 54000, 0.000001] loss: 0.000003988\n",
      "[5, 56000, 0.000001] loss: 0.000004031\n",
      "[5, 58000, 0.000001] loss: 0.000003824\n",
      "[5, 60000, 0.000001] loss: 0.000004343\n",
      "[5, 62000, 0.000001] loss: 0.000003725\n",
      "[5, 64000, 0.000001] loss: 0.000003860\n",
      "[5, 66000, 0.000001] loss: 0.000003856\n",
      "[5, 68000, 0.000001] loss: 0.000004053\n",
      "[5, 70000, 0.000001] loss: 0.000003957\n",
      "[5, 72000, 0.000001] loss: 0.000004070\n",
      "[5, 74000, 0.000001] loss: 0.000003868\n",
      "[5, 76000, 0.000001] loss: 0.000004245\n",
      "[5, 78000, 0.000001] loss: 0.000004042\n",
      "[5, 80000, 0.000001] loss: 0.000003819\n",
      "[5, 82000, 0.000001] loss: 0.000004077\n",
      "[5, 84000, 0.000001] loss: 0.000004333\n",
      "[5, 86000, 0.000001] loss: 0.000003839\n",
      "[5, 88000, 0.000001] loss: 0.000003798\n",
      "[5, 90000, 0.000001] loss: 0.000003938\n",
      "[5, 92000, 0.000001] loss: 0.000003828\n",
      "[5, 94000, 0.000001] loss: 0.000004066\n",
      "[5, 96000, 0.000001] loss: 0.000003999\n",
      "[5, 98000, 0.000001] loss: 0.000004249\n",
      "[5, 100000, 0.000001] loss: 0.000003921\n",
      "[5, 102000, 0.000001] loss: 0.000004071\n",
      "[5, 104000, 0.000001] loss: 0.000003577\n",
      "[5, 106000, 0.000001] loss: 0.000004109\n",
      "[5, 108000, 0.000001] loss: 0.000003940\n",
      "[5, 110000, 0.000001] loss: 0.000004079\n",
      "[5, 112000, 0.000001] loss: 0.000003723\n",
      "[5, 114000, 0.000001] loss: 0.000003845\n",
      "[5, 116000, 0.000001] loss: 0.000003700\n",
      "[5, 118000, 0.000001] loss: 0.000003850\n",
      "[5, 120000, 0.000001] loss: 0.000004155\n",
      "[5, 122000, 0.000001] loss: 0.000003674\n",
      "[5, 124000, 0.000001] loss: 0.000003962\n",
      "[5, 126000, 0.000001] loss: 0.000003886\n",
      "[5, 128000, 0.000001] loss: 0.000003867\n",
      "[5, 130000, 0.000001] loss: 0.000003896\n",
      "[5, 132000, 0.000001] loss: 0.000003927\n",
      "[5, 134000, 0.000001] loss: 0.000003945\n",
      "[5, 136000, 0.000001] loss: 0.000003772\n",
      "[5, 138000, 0.000001] loss: 0.000003869\n",
      "[5, 140000, 0.000001] loss: 0.000003894\n",
      "[5, 142000, 0.000001] loss: 0.000003833\n",
      "[5, 144000, 0.000001] loss: 0.000004079\n",
      "[5, 146000, 0.000001] loss: 0.000003879\n",
      "[5, 148000, 0.000001] loss: 0.000003879\n",
      "[5, 150000, 0.000001] loss: 0.000003891\n",
      "[5, 152000, 0.000001] loss: 0.000004087\n",
      "[5, 154000, 0.000001] loss: 0.000004184\n",
      "[5, 156000, 0.000001] loss: 0.000004144\n",
      "[5, 158000, 0.000001] loss: 0.000003819\n",
      "[5, 160000, 0.000001] loss: 0.000003720\n",
      "[5, 162000, 0.000001] loss: 0.000003993\n",
      "[5, 164000, 0.000001] loss: 0.000003668\n",
      "[5, 166000, 0.000001] loss: 0.000003803\n",
      "[5, 168000, 0.000001] loss: 0.000003797\n",
      "[5, 170000, 0.000001] loss: 0.000003680\n",
      "[5, 172000, 0.000001] loss: 0.000004038\n",
      "[5, 174000, 0.000001] loss: 0.000003913\n",
      "[5, 176000, 0.000001] loss: 0.000003950\n",
      "Finish Training Theta Parameters\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "thetaOptnet = OptNet()\n",
    "thetaOptnet.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(thetaOptnet.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(y_trainTh)):\n",
    "        inputs = X_trainTh[i].to(device)\n",
    "        labels = y_trainTh[i].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = thetaOptnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d, %6f] loss: %.9f' %\n",
    "                  (epoch + 1, i + 1, optimizer.param_groups[0]['lr'],running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Finish Training Theta Parameters\")\n",
    "# save the training model\n",
    "theta_model_PATH = './model/Theta_net.pth'\n",
    "torch.save(thetaOptnet.state_dict(), theta_model_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training on the $\\phi_{targ}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. train the $\\phi_{targ}$ model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "X_trainPh = torch.tensor(train_X.to_numpy()).float()\n",
    "y_trainPh = torch.tensor(train_y['targCalPh'].to_numpy()).float()\n",
    "X_testPh  = torch.tensor(test_X.to_numpy()).float()\n",
    "y_testPh  = torch.tensor(test_y['targCalPh'].to_numpy()).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000, 0.010000] loss: 0.000249779\n",
      "[1,  4000, 0.010000] loss: 0.000011703\n",
      "[1,  6000, 0.010000] loss: 0.000020176\n",
      "[1,  8000, 0.010000] loss: 0.000015729\n",
      "[1, 10000, 0.010000] loss: 0.000013237\n",
      "[1, 12000, 0.010000] loss: 0.000011413\n",
      "[1, 14000, 0.010000] loss: 0.000011132\n",
      "[1, 16000, 0.010000] loss: 0.000010750\n",
      "[1, 18000, 0.010000] loss: 0.000010036\n",
      "[1, 20000, 0.010000] loss: 0.000010075\n",
      "[1, 22000, 0.010000] loss: 0.000008916\n",
      "[1, 24000, 0.010000] loss: 0.000008377\n",
      "[1, 26000, 0.010000] loss: 0.000009205\n",
      "[1, 28000, 0.010000] loss: 0.000008966\n",
      "[1, 30000, 0.010000] loss: 0.000008907\n",
      "[1, 32000, 0.010000] loss: 0.000010922\n",
      "[1, 34000, 0.010000] loss: 0.000008362\n",
      "[1, 36000, 0.010000] loss: 0.000008454\n",
      "[1, 38000, 0.010000] loss: 0.000008496\n",
      "[1, 40000, 0.010000] loss: 0.000008915\n",
      "[1, 42000, 0.010000] loss: 0.000008648\n",
      "[1, 44000, 0.010000] loss: 0.000007824\n",
      "[1, 46000, 0.010000] loss: 0.000010606\n",
      "[1, 48000, 0.010000] loss: 0.000008387\n",
      "[1, 50000, 0.010000] loss: 0.000008054\n",
      "[1, 52000, 0.010000] loss: 0.000008365\n",
      "[1, 54000, 0.010000] loss: 0.000008055\n",
      "[1, 56000, 0.010000] loss: 0.000007377\n",
      "[1, 58000, 0.010000] loss: 0.000008566\n",
      "[1, 60000, 0.010000] loss: 0.000009212\n",
      "[1, 62000, 0.010000] loss: 0.000009237\n",
      "[1, 64000, 0.010000] loss: 0.000008363\n",
      "[1, 66000, 0.010000] loss: 0.000008081\n",
      "[1, 68000, 0.010000] loss: 0.000007919\n",
      "[1, 70000, 0.010000] loss: 0.000007693\n",
      "[1, 72000, 0.010000] loss: 0.000008346\n",
      "[1, 74000, 0.010000] loss: 0.000008685\n",
      "[1, 76000, 0.010000] loss: 0.000008557\n",
      "[1, 78000, 0.010000] loss: 0.000008012\n",
      "[1, 80000, 0.010000] loss: 0.000008531\n",
      "[1, 82000, 0.010000] loss: 0.000007451\n",
      "[1, 84000, 0.010000] loss: 0.000007689\n",
      "[1, 86000, 0.010000] loss: 0.000007757\n",
      "[1, 88000, 0.010000] loss: 0.000008395\n",
      "[1, 90000, 0.010000] loss: 0.000007685\n",
      "[1, 92000, 0.010000] loss: 0.000008246\n",
      "[1, 94000, 0.010000] loss: 0.000008055\n",
      "[1, 96000, 0.010000] loss: 0.000009230\n",
      "[1, 98000, 0.010000] loss: 0.000008377\n",
      "[1, 100000, 0.010000] loss: 0.000008579\n",
      "[1, 102000, 0.010000] loss: 0.000007351\n",
      "[1, 104000, 0.010000] loss: 0.000009382\n",
      "[1, 106000, 0.010000] loss: 0.000007669\n",
      "[1, 108000, 0.010000] loss: 0.000008307\n",
      "[1, 110000, 0.010000] loss: 0.000007470\n",
      "[1, 112000, 0.010000] loss: 0.000008172\n",
      "[1, 114000, 0.010000] loss: 0.000008951\n",
      "[1, 116000, 0.010000] loss: 0.000007350\n",
      "[1, 118000, 0.010000] loss: 0.000008081\n",
      "[1, 120000, 0.010000] loss: 0.000007723\n",
      "[1, 122000, 0.010000] loss: 0.000007532\n",
      "[1, 124000, 0.010000] loss: 0.000007677\n",
      "[1, 126000, 0.010000] loss: 0.000007485\n",
      "[1, 128000, 0.010000] loss: 0.000006961\n",
      "[1, 130000, 0.010000] loss: 0.000008100\n",
      "[1, 132000, 0.010000] loss: 0.000007500\n",
      "[1, 134000, 0.010000] loss: 0.000007541\n",
      "[1, 136000, 0.010000] loss: 0.000007001\n",
      "[1, 138000, 0.010000] loss: 0.000006750\n",
      "[1, 140000, 0.010000] loss: 0.000008624\n",
      "[1, 142000, 0.010000] loss: 0.000006669\n",
      "[1, 144000, 0.010000] loss: 0.000008084\n",
      "[1, 146000, 0.010000] loss: 0.000006767\n",
      "[1, 148000, 0.010000] loss: 0.000007476\n",
      "[1, 150000, 0.010000] loss: 0.000007334\n",
      "[1, 152000, 0.010000] loss: 0.000007147\n",
      "[1, 154000, 0.010000] loss: 0.000008709\n",
      "[1, 156000, 0.010000] loss: 0.000006956\n",
      "[1, 158000, 0.010000] loss: 0.000006893\n",
      "[1, 160000, 0.010000] loss: 0.000007757\n",
      "[1, 162000, 0.010000] loss: 0.000007539\n",
      "[1, 164000, 0.010000] loss: 0.000007447\n",
      "[1, 166000, 0.010000] loss: 0.000007794\n",
      "[1, 168000, 0.010000] loss: 0.000007073\n",
      "[1, 170000, 0.010000] loss: 0.000007653\n",
      "[1, 172000, 0.010000] loss: 0.000007793\n",
      "[1, 174000, 0.010000] loss: 0.000007728\n",
      "[1, 176000, 0.010000] loss: 0.000007235\n",
      "[2,  2000, 0.001000] loss: 0.000002286\n",
      "[2,  4000, 0.001000] loss: 0.000002330\n",
      "[2,  6000, 0.001000] loss: 0.000002299\n",
      "[2,  8000, 0.001000] loss: 0.000002508\n",
      "[2, 10000, 0.001000] loss: 0.000002277\n",
      "[2, 12000, 0.001000] loss: 0.000002418\n",
      "[2, 14000, 0.001000] loss: 0.000002278\n",
      "[2, 16000, 0.001000] loss: 0.000002391\n",
      "[2, 18000, 0.001000] loss: 0.000002225\n",
      "[2, 20000, 0.001000] loss: 0.000002516\n",
      "[2, 22000, 0.001000] loss: 0.000002303\n",
      "[2, 24000, 0.001000] loss: 0.000002240\n",
      "[2, 26000, 0.001000] loss: 0.000002285\n",
      "[2, 28000, 0.001000] loss: 0.000002227\n",
      "[2, 30000, 0.001000] loss: 0.000002258\n",
      "[2, 32000, 0.001000] loss: 0.000002374\n",
      "[2, 34000, 0.001000] loss: 0.000002283\n",
      "[2, 36000, 0.001000] loss: 0.000002323\n",
      "[2, 38000, 0.001000] loss: 0.000002479\n",
      "[2, 40000, 0.001000] loss: 0.000002107\n",
      "[2, 42000, 0.001000] loss: 0.000002137\n",
      "[2, 44000, 0.001000] loss: 0.000002221\n",
      "[2, 46000, 0.001000] loss: 0.000002273\n",
      "[2, 48000, 0.001000] loss: 0.000002379\n",
      "[2, 50000, 0.001000] loss: 0.000002199\n",
      "[2, 52000, 0.001000] loss: 0.000002413\n",
      "[2, 54000, 0.001000] loss: 0.000002368\n",
      "[2, 56000, 0.001000] loss: 0.000002281\n",
      "[2, 58000, 0.001000] loss: 0.000002091\n",
      "[2, 60000, 0.001000] loss: 0.000002133\n",
      "[2, 62000, 0.001000] loss: 0.000002405\n",
      "[2, 64000, 0.001000] loss: 0.000002258\n",
      "[2, 66000, 0.001000] loss: 0.000002130\n",
      "[2, 68000, 0.001000] loss: 0.000002208\n",
      "[2, 70000, 0.001000] loss: 0.000002290\n",
      "[2, 72000, 0.001000] loss: 0.000002194\n",
      "[2, 74000, 0.001000] loss: 0.000002452\n",
      "[2, 76000, 0.001000] loss: 0.000002169\n",
      "[2, 78000, 0.001000] loss: 0.000002231\n",
      "[2, 80000, 0.001000] loss: 0.000002326\n",
      "[2, 82000, 0.001000] loss: 0.000002158\n",
      "[2, 84000, 0.001000] loss: 0.000002227\n",
      "[2, 86000, 0.001000] loss: 0.000002019\n",
      "[2, 88000, 0.001000] loss: 0.000002151\n",
      "[2, 90000, 0.001000] loss: 0.000002216\n",
      "[2, 92000, 0.001000] loss: 0.000002140\n",
      "[2, 94000, 0.001000] loss: 0.000002133\n",
      "[2, 96000, 0.001000] loss: 0.000002167\n",
      "[2, 98000, 0.001000] loss: 0.000002186\n",
      "[2, 100000, 0.001000] loss: 0.000002268\n",
      "[2, 102000, 0.001000] loss: 0.000002107\n",
      "[2, 104000, 0.001000] loss: 0.000002196\n",
      "[2, 106000, 0.001000] loss: 0.000002244\n",
      "[2, 108000, 0.001000] loss: 0.000002207\n",
      "[2, 110000, 0.001000] loss: 0.000002166\n",
      "[2, 112000, 0.001000] loss: 0.000002205\n",
      "[2, 114000, 0.001000] loss: 0.000002173\n",
      "[2, 116000, 0.001000] loss: 0.000002080\n",
      "[2, 118000, 0.001000] loss: 0.000002254\n",
      "[2, 120000, 0.001000] loss: 0.000002135\n",
      "[2, 122000, 0.001000] loss: 0.000002129\n",
      "[2, 124000, 0.001000] loss: 0.000002238\n",
      "[2, 126000, 0.001000] loss: 0.000002100\n",
      "[2, 128000, 0.001000] loss: 0.000002074\n",
      "[2, 130000, 0.001000] loss: 0.000002228\n",
      "[2, 132000, 0.001000] loss: 0.000002157\n",
      "[2, 134000, 0.001000] loss: 0.000002183\n",
      "[2, 136000, 0.001000] loss: 0.000002106\n",
      "[2, 138000, 0.001000] loss: 0.000002053\n",
      "[2, 140000, 0.001000] loss: 0.000002103\n",
      "[2, 142000, 0.001000] loss: 0.000002271\n",
      "[2, 144000, 0.001000] loss: 0.000002263\n",
      "[2, 146000, 0.001000] loss: 0.000002049\n",
      "[2, 148000, 0.001000] loss: 0.000002140\n",
      "[2, 150000, 0.001000] loss: 0.000002063\n",
      "[2, 152000, 0.001000] loss: 0.000001973\n",
      "[2, 154000, 0.001000] loss: 0.000002326\n",
      "[2, 156000, 0.001000] loss: 0.000002186\n",
      "[2, 158000, 0.001000] loss: 0.000002237\n",
      "[2, 160000, 0.001000] loss: 0.000002021\n",
      "[2, 162000, 0.001000] loss: 0.000002128\n",
      "[2, 164000, 0.001000] loss: 0.000002007\n",
      "[2, 166000, 0.001000] loss: 0.000002226\n",
      "[2, 168000, 0.001000] loss: 0.000002107\n",
      "[2, 170000, 0.001000] loss: 0.000002045\n",
      "[2, 172000, 0.001000] loss: 0.000002039\n",
      "[2, 174000, 0.001000] loss: 0.000002174\n",
      "[2, 176000, 0.001000] loss: 0.000001983\n",
      "[3,  2000, 0.000100] loss: 0.000001603\n",
      "[3,  4000, 0.000100] loss: 0.000001607\n",
      "[3,  6000, 0.000100] loss: 0.000001564\n",
      "[3,  8000, 0.000100] loss: 0.000001715\n",
      "[3, 10000, 0.000100] loss: 0.000001519\n",
      "[3, 12000, 0.000100] loss: 0.000001591\n",
      "[3, 14000, 0.000100] loss: 0.000001553\n",
      "[3, 16000, 0.000100] loss: 0.000001609\n",
      "[3, 18000, 0.000100] loss: 0.000001569\n",
      "[3, 20000, 0.000100] loss: 0.000001724\n",
      "[3, 22000, 0.000100] loss: 0.000001586\n",
      "[3, 24000, 0.000100] loss: 0.000001552\n",
      "[3, 26000, 0.000100] loss: 0.000001597\n",
      "[3, 28000, 0.000100] loss: 0.000001539\n",
      "[3, 30000, 0.000100] loss: 0.000001585\n",
      "[3, 32000, 0.000100] loss: 0.000001579\n",
      "[3, 34000, 0.000100] loss: 0.000001566\n",
      "[3, 36000, 0.000100] loss: 0.000001523\n",
      "[3, 38000, 0.000100] loss: 0.000001606\n",
      "[3, 40000, 0.000100] loss: 0.000001444\n",
      "[3, 42000, 0.000100] loss: 0.000001527\n",
      "[3, 44000, 0.000100] loss: 0.000001545\n",
      "[3, 46000, 0.000100] loss: 0.000001586\n",
      "[3, 48000, 0.000100] loss: 0.000001586\n",
      "[3, 50000, 0.000100] loss: 0.000001575\n",
      "[3, 52000, 0.000100] loss: 0.000001659\n",
      "[3, 54000, 0.000100] loss: 0.000001634\n",
      "[3, 56000, 0.000100] loss: 0.000001528\n",
      "[3, 58000, 0.000100] loss: 0.000001564\n",
      "[3, 60000, 0.000100] loss: 0.000001446\n",
      "[3, 62000, 0.000100] loss: 0.000001606\n",
      "[3, 64000, 0.000100] loss: 0.000001590\n",
      "[3, 66000, 0.000100] loss: 0.000001555\n",
      "[3, 68000, 0.000100] loss: 0.000001571\n",
      "[3, 70000, 0.000100] loss: 0.000001629\n",
      "[3, 72000, 0.000100] loss: 0.000001554\n",
      "[3, 74000, 0.000100] loss: 0.000001655\n",
      "[3, 76000, 0.000100] loss: 0.000001575\n",
      "[3, 78000, 0.000100] loss: 0.000001588\n",
      "[3, 80000, 0.000100] loss: 0.000001566\n",
      "[3, 82000, 0.000100] loss: 0.000001511\n",
      "[3, 84000, 0.000100] loss: 0.000001568\n",
      "[3, 86000, 0.000100] loss: 0.000001404\n",
      "[3, 88000, 0.000100] loss: 0.000001512\n",
      "[3, 90000, 0.000100] loss: 0.000001587\n",
      "[3, 92000, 0.000100] loss: 0.000001559\n",
      "[3, 94000, 0.000100] loss: 0.000001518\n",
      "[3, 96000, 0.000100] loss: 0.000001623\n",
      "[3, 98000, 0.000100] loss: 0.000001527\n",
      "[3, 100000, 0.000100] loss: 0.000001645\n",
      "[3, 102000, 0.000100] loss: 0.000001547\n",
      "[3, 104000, 0.000100] loss: 0.000001520\n",
      "[3, 106000, 0.000100] loss: 0.000001605\n",
      "[3, 108000, 0.000100] loss: 0.000001616\n",
      "[3, 110000, 0.000100] loss: 0.000001567\n",
      "[3, 112000, 0.000100] loss: 0.000001541\n",
      "[3, 114000, 0.000100] loss: 0.000001529\n",
      "[3, 116000, 0.000100] loss: 0.000001529\n",
      "[3, 118000, 0.000100] loss: 0.000001565\n",
      "[3, 120000, 0.000100] loss: 0.000001583\n",
      "[3, 122000, 0.000100] loss: 0.000001524\n",
      "[3, 124000, 0.000100] loss: 0.000001638\n",
      "[3, 126000, 0.000100] loss: 0.000001535\n",
      "[3, 128000, 0.000100] loss: 0.000001502\n",
      "[3, 130000, 0.000100] loss: 0.000001551\n",
      "[3, 132000, 0.000100] loss: 0.000001541\n",
      "[3, 134000, 0.000100] loss: 0.000001611\n",
      "[3, 136000, 0.000100] loss: 0.000001571\n",
      "[3, 138000, 0.000100] loss: 0.000001488\n",
      "[3, 140000, 0.000100] loss: 0.000001504\n",
      "[3, 142000, 0.000100] loss: 0.000001541\n",
      "[3, 144000, 0.000100] loss: 0.000001560\n",
      "[3, 146000, 0.000100] loss: 0.000001517\n",
      "[3, 148000, 0.000100] loss: 0.000001554\n",
      "[3, 150000, 0.000100] loss: 0.000001483\n",
      "[3, 152000, 0.000100] loss: 0.000001466\n",
      "[3, 154000, 0.000100] loss: 0.000001658\n",
      "[3, 156000, 0.000100] loss: 0.000001537\n",
      "[3, 158000, 0.000100] loss: 0.000001597\n",
      "[3, 160000, 0.000100] loss: 0.000001483\n",
      "[3, 162000, 0.000100] loss: 0.000001563\n",
      "[3, 164000, 0.000100] loss: 0.000001511\n",
      "[3, 166000, 0.000100] loss: 0.000001611\n",
      "[3, 168000, 0.000100] loss: 0.000001544\n",
      "[3, 170000, 0.000100] loss: 0.000001555\n",
      "[3, 172000, 0.000100] loss: 0.000001516\n",
      "[3, 174000, 0.000100] loss: 0.000001613\n",
      "[3, 176000, 0.000100] loss: 0.000001463\n",
      "[4,  2000, 0.000010] loss: 0.000001525\n",
      "[4,  4000, 0.000010] loss: 0.000001507\n",
      "[4,  6000, 0.000010] loss: 0.000001483\n",
      "[4,  8000, 0.000010] loss: 0.000001641\n",
      "[4, 10000, 0.000010] loss: 0.000001441\n",
      "[4, 12000, 0.000010] loss: 0.000001505\n",
      "[4, 14000, 0.000010] loss: 0.000001482\n",
      "[4, 16000, 0.000010] loss: 0.000001512\n",
      "[4, 18000, 0.000010] loss: 0.000001502\n",
      "[4, 20000, 0.000010] loss: 0.000001633\n",
      "[4, 22000, 0.000010] loss: 0.000001496\n",
      "[4, 24000, 0.000010] loss: 0.000001459\n",
      "[4, 26000, 0.000010] loss: 0.000001517\n",
      "[4, 28000, 0.000010] loss: 0.000001475\n",
      "[4, 30000, 0.000010] loss: 0.000001502\n",
      "[4, 32000, 0.000010] loss: 0.000001513\n",
      "[4, 34000, 0.000010] loss: 0.000001490\n",
      "[4, 36000, 0.000010] loss: 0.000001441\n",
      "[4, 38000, 0.000010] loss: 0.000001517\n",
      "[4, 40000, 0.000010] loss: 0.000001360\n",
      "[4, 42000, 0.000010] loss: 0.000001457\n",
      "[4, 44000, 0.000010] loss: 0.000001465\n",
      "[4, 46000, 0.000010] loss: 0.000001507\n",
      "[4, 48000, 0.000010] loss: 0.000001512\n",
      "[4, 50000, 0.000010] loss: 0.000001493\n",
      "[4, 52000, 0.000010] loss: 0.000001589\n",
      "[4, 54000, 0.000010] loss: 0.000001561\n",
      "[4, 56000, 0.000010] loss: 0.000001465\n",
      "[4, 58000, 0.000010] loss: 0.000001479\n",
      "[4, 60000, 0.000010] loss: 0.000001381\n",
      "[4, 62000, 0.000010] loss: 0.000001518\n",
      "[4, 64000, 0.000010] loss: 0.000001504\n",
      "[4, 66000, 0.000010] loss: 0.000001476\n",
      "[4, 68000, 0.000010] loss: 0.000001486\n",
      "[4, 70000, 0.000010] loss: 0.000001549\n",
      "[4, 72000, 0.000010] loss: 0.000001487\n",
      "[4, 74000, 0.000010] loss: 0.000001583\n",
      "[4, 76000, 0.000010] loss: 0.000001503\n",
      "[4, 78000, 0.000010] loss: 0.000001521\n",
      "[4, 80000, 0.000010] loss: 0.000001494\n",
      "[4, 82000, 0.000010] loss: 0.000001453\n",
      "[4, 84000, 0.000010] loss: 0.000001477\n",
      "[4, 86000, 0.000010] loss: 0.000001348\n",
      "[4, 88000, 0.000010] loss: 0.000001429\n",
      "[4, 90000, 0.000010] loss: 0.000001510\n",
      "[4, 92000, 0.000010] loss: 0.000001475\n",
      "[4, 94000, 0.000010] loss: 0.000001455\n",
      "[4, 96000, 0.000010] loss: 0.000001545\n",
      "[4, 98000, 0.000010] loss: 0.000001469\n",
      "[4, 100000, 0.000010] loss: 0.000001562\n",
      "[4, 102000, 0.000010] loss: 0.000001485\n",
      "[4, 104000, 0.000010] loss: 0.000001442\n",
      "[4, 106000, 0.000010] loss: 0.000001544\n",
      "[4, 108000, 0.000010] loss: 0.000001558\n",
      "[4, 110000, 0.000010] loss: 0.000001496\n",
      "[4, 112000, 0.000010] loss: 0.000001463\n",
      "[4, 114000, 0.000010] loss: 0.000001460\n",
      "[4, 116000, 0.000010] loss: 0.000001473\n",
      "[4, 118000, 0.000010] loss: 0.000001488\n",
      "[4, 120000, 0.000010] loss: 0.000001521\n",
      "[4, 122000, 0.000010] loss: 0.000001447\n",
      "[4, 124000, 0.000010] loss: 0.000001542\n",
      "[4, 126000, 0.000010] loss: 0.000001464\n",
      "[4, 128000, 0.000010] loss: 0.000001441\n",
      "[4, 130000, 0.000010] loss: 0.000001482\n",
      "[4, 132000, 0.000010] loss: 0.000001481\n",
      "[4, 134000, 0.000010] loss: 0.000001550\n",
      "[4, 136000, 0.000010] loss: 0.000001496\n",
      "[4, 138000, 0.000010] loss: 0.000001417\n",
      "[4, 140000, 0.000010] loss: 0.000001441\n",
      "[4, 142000, 0.000010] loss: 0.000001469\n",
      "[4, 144000, 0.000010] loss: 0.000001493\n",
      "[4, 146000, 0.000010] loss: 0.000001445\n",
      "[4, 148000, 0.000010] loss: 0.000001498\n",
      "[4, 150000, 0.000010] loss: 0.000001425\n",
      "[4, 152000, 0.000010] loss: 0.000001398\n",
      "[4, 154000, 0.000010] loss: 0.000001583\n",
      "[4, 156000, 0.000010] loss: 0.000001486\n",
      "[4, 158000, 0.000010] loss: 0.000001536\n",
      "[4, 160000, 0.000010] loss: 0.000001406\n",
      "[4, 162000, 0.000010] loss: 0.000001494\n",
      "[4, 164000, 0.000010] loss: 0.000001456\n",
      "[4, 166000, 0.000010] loss: 0.000001555\n",
      "[4, 168000, 0.000010] loss: 0.000001479\n",
      "[4, 170000, 0.000010] loss: 0.000001489\n",
      "[4, 172000, 0.000010] loss: 0.000001445\n",
      "[4, 174000, 0.000010] loss: 0.000001536\n",
      "[4, 176000, 0.000010] loss: 0.000001407\n",
      "[5,  2000, 0.000001] loss: 0.000001519\n",
      "[5,  4000, 0.000001] loss: 0.000001498\n",
      "[5,  6000, 0.000001] loss: 0.000001473\n",
      "[5,  8000, 0.000001] loss: 0.000001629\n",
      "[5, 10000, 0.000001] loss: 0.000001436\n",
      "[5, 12000, 0.000001] loss: 0.000001498\n",
      "[5, 14000, 0.000001] loss: 0.000001473\n",
      "[5, 16000, 0.000001] loss: 0.000001504\n",
      "[5, 18000, 0.000001] loss: 0.000001493\n",
      "[5, 20000, 0.000001] loss: 0.000001620\n",
      "[5, 22000, 0.000001] loss: 0.000001491\n",
      "[5, 24000, 0.000001] loss: 0.000001452\n",
      "[5, 26000, 0.000001] loss: 0.000001511\n",
      "[5, 28000, 0.000001] loss: 0.000001466\n",
      "[5, 30000, 0.000001] loss: 0.000001493\n",
      "[5, 32000, 0.000001] loss: 0.000001504\n",
      "[5, 34000, 0.000001] loss: 0.000001481\n",
      "[5, 36000, 0.000001] loss: 0.000001437\n",
      "[5, 38000, 0.000001] loss: 0.000001510\n",
      "[5, 40000, 0.000001] loss: 0.000001350\n",
      "[5, 42000, 0.000001] loss: 0.000001449\n",
      "[5, 44000, 0.000001] loss: 0.000001456\n",
      "[5, 46000, 0.000001] loss: 0.000001496\n",
      "[5, 48000, 0.000001] loss: 0.000001504\n",
      "[5, 50000, 0.000001] loss: 0.000001484\n",
      "[5, 52000, 0.000001] loss: 0.000001584\n",
      "[5, 54000, 0.000001] loss: 0.000001551\n",
      "[5, 56000, 0.000001] loss: 0.000001462\n",
      "[5, 58000, 0.000001] loss: 0.000001467\n",
      "[5, 60000, 0.000001] loss: 0.000001377\n",
      "[5, 62000, 0.000001] loss: 0.000001512\n",
      "[5, 64000, 0.000001] loss: 0.000001495\n",
      "[5, 66000, 0.000001] loss: 0.000001467\n",
      "[5, 68000, 0.000001] loss: 0.000001480\n",
      "[5, 70000, 0.000001] loss: 0.000001545\n",
      "[5, 72000, 0.000001] loss: 0.000001476\n",
      "[5, 74000, 0.000001] loss: 0.000001576\n",
      "[5, 76000, 0.000001] loss: 0.000001495\n",
      "[5, 78000, 0.000001] loss: 0.000001510\n",
      "[5, 80000, 0.000001] loss: 0.000001486\n",
      "[5, 82000, 0.000001] loss: 0.000001445\n",
      "[5, 84000, 0.000001] loss: 0.000001473\n",
      "[5, 86000, 0.000001] loss: 0.000001338\n",
      "[5, 88000, 0.000001] loss: 0.000001421\n",
      "[5, 90000, 0.000001] loss: 0.000001504\n",
      "[5, 92000, 0.000001] loss: 0.000001471\n",
      "[5, 94000, 0.000001] loss: 0.000001452\n",
      "[5, 96000, 0.000001] loss: 0.000001537\n",
      "[5, 98000, 0.000001] loss: 0.000001467\n",
      "[5, 100000, 0.000001] loss: 0.000001553\n",
      "[5, 102000, 0.000001] loss: 0.000001477\n",
      "[5, 104000, 0.000001] loss: 0.000001432\n",
      "[5, 106000, 0.000001] loss: 0.000001535\n",
      "[5, 108000, 0.000001] loss: 0.000001553\n",
      "[5, 110000, 0.000001] loss: 0.000001485\n",
      "[5, 112000, 0.000001] loss: 0.000001453\n",
      "[5, 114000, 0.000001] loss: 0.000001454\n",
      "[5, 116000, 0.000001] loss: 0.000001472\n",
      "[5, 118000, 0.000001] loss: 0.000001476\n",
      "[5, 120000, 0.000001] loss: 0.000001512\n",
      "[5, 122000, 0.000001] loss: 0.000001439\n",
      "[5, 124000, 0.000001] loss: 0.000001536\n",
      "[5, 126000, 0.000001] loss: 0.000001457\n",
      "[5, 128000, 0.000001] loss: 0.000001432\n",
      "[5, 130000, 0.000001] loss: 0.000001475\n",
      "[5, 132000, 0.000001] loss: 0.000001473\n",
      "[5, 134000, 0.000001] loss: 0.000001540\n",
      "[5, 136000, 0.000001] loss: 0.000001490\n",
      "[5, 138000, 0.000001] loss: 0.000001409\n",
      "[5, 140000, 0.000001] loss: 0.000001435\n",
      "[5, 142000, 0.000001] loss: 0.000001461\n",
      "[5, 144000, 0.000001] loss: 0.000001482\n",
      "[5, 146000, 0.000001] loss: 0.000001438\n",
      "[5, 148000, 0.000001] loss: 0.000001494\n",
      "[5, 150000, 0.000001] loss: 0.000001421\n",
      "[5, 152000, 0.000001] loss: 0.000001390\n",
      "[5, 154000, 0.000001] loss: 0.000001575\n",
      "[5, 156000, 0.000001] loss: 0.000001481\n",
      "[5, 158000, 0.000001] loss: 0.000001529\n",
      "[5, 160000, 0.000001] loss: 0.000001399\n",
      "[5, 162000, 0.000001] loss: 0.000001489\n",
      "[5, 164000, 0.000001] loss: 0.000001457\n",
      "[5, 166000, 0.000001] loss: 0.000001545\n",
      "[5, 168000, 0.000001] loss: 0.000001472\n",
      "[5, 170000, 0.000001] loss: 0.000001481\n",
      "[5, 172000, 0.000001] loss: 0.000001438\n",
      "[5, 174000, 0.000001] loss: 0.000001528\n",
      "[5, 176000, 0.000001] loss: 0.000001400\n",
      "Finish Training Theta Parameters\n"
     ]
    }
   ],
   "source": [
    "phiOptnet = OptNet()\n",
    "phiOptnet.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(phiOptnet.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(y_trainPh)):\n",
    "        inputs = X_trainPh[i].to(device)\n",
    "        labels = y_trainPh[i].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs =  phiOptnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d, %6f] loss: %.9f' %\n",
    "                  (epoch + 1, i + 1, optimizer.param_groups[0]['lr'],running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Finish Training Theta Parameters\")\n",
    "# save the training model\n",
    "phi_model_PATH = './model/Phi_net.pth'\n",
    "torch.save(phiOptnet.state_dict(), phi_model_PATH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the Training model\n",
    "\n",
    "### 1. Seperate Test on the $\\theta$ and $\\phi$ on the test dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "testNet_theta = OptNet()\n",
    "testNet_theta.load_state_dict(torch.load(theta_model_PATH))\n",
    "\n",
    "test_loss = 0\n",
    "mseArr = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(y_testTh)):\n",
    "        positions = X_testTh[i]\n",
    "        labels = y_testTh[i]\n",
    "\n",
    "        outputs = testNet_theta(positions)\n",
    "        loss = criterion(outputs,labels)\n",
    "        test_loss += loss\n",
    "        mseArr.append(loss)\n",
    "print('test_error is :{}   / {}'.format(test_loss,sum(mseArr)/len(mseArr)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_error is :0.17471259832382202   / 3.94394010072574e-06\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_error is :0.06580284982919693   / 1.4854251730866963e-06\n"
     ]
    }
   ],
   "source": [
    "testNet_phi = OptNet()\n",
    "testNet_phi.load_state_dict(torch.load(phi_model_PATH))\n",
    "\n",
    "test_loss = 0\n",
    "mseArr = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(y_testPh)):\n",
    "        positions = X_testPh[i]\n",
    "        labels = y_testPh[i]\n",
    "        outputs = testNet_phi(positions)\n",
    "        loss = criterion(outputs,labels)\n",
    "        test_loss += loss\n",
    "        mseArr.append(loss)\n",
    "print('test_error is :{}   / {}'.format(test_loss,sum(mseArr)/len(mseArr)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2D histogram distribution test (Include all the training and test dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "def predictor(model_path,x_test):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = OptNet()\n",
    "    net.load_state_dict(torch.load(model_path))\n",
    "    net.to(device)\n",
    "    res = torch.empty(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(x_test)):\n",
    "            positions = x_test[i].to(device)\n",
    "            outputs = net(positions).cpu()\n",
    "            res = torch.cat((res,outputs))\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newdriver/Storage/HomeDir/Learning/spectrometer_nn/training_nn/venv/lib/python3.9/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "       x0th0y0ph0  x0th0y0ph1    x0th0y0ph2  x0th0y1ph0  x0th0y1ph1  \\\n0               1   -0.002624  6.886320e-06    0.010331   -0.000027   \n1               1   -0.008738  7.634650e-05    0.005508   -0.000048   \n2               1   -0.000955  9.125640e-07    0.012355   -0.000012   \n3               1   -0.001454  2.114490e-06    0.007224   -0.000011   \n4               1   -0.002899  8.404720e-06    0.011637   -0.000034   \n...           ...         ...           ...         ...         ...   \n32890           1    0.006142  3.771990e-05   -0.023787   -0.000146   \n32891           1    0.010942  1.197270e-04   -0.024216   -0.000265   \n32892           1    0.010258  1.052250e-04   -0.023374   -0.000240   \n32893           1    0.005959  3.551330e-05   -0.022421   -0.000134   \n32894           1    0.007339  5.386610e-05   -0.021958   -0.000161   \n\n       x0th0y2ph0  x0th1y0ph0    x0th1y0ph1  x0th1y1ph0    x0th2y0ph0  \\\n0        0.000107   -0.002354  6.176320e-06   -0.000024  5.539530e-06   \n1        0.000030   -0.008940  7.811040e-05   -0.000049  7.991500e-05   \n2        0.000153   -0.001000  9.553490e-07   -0.000012  1.000140e-06   \n3        0.000052    0.000248 -3.604800e-07    0.000002  6.145490e-08   \n4        0.000135   -0.002637  7.645830e-06   -0.000031  6.955460e-06   \n...           ...         ...           ...         ...           ...   \n32890    0.000566    0.006353  3.901530e-05   -0.000151  4.035530e-05   \n32891    0.000586    0.003000  3.282320e-05   -0.000073  8.998440e-06   \n32892    0.000546    0.003103  3.183400e-05   -0.000073  9.630840e-06   \n32893    0.000503    0.007686  4.580190e-05   -0.000172  5.907120e-05   \n32894    0.000482    0.007450  5.467950e-05   -0.000164  5.550520e-05   \n\n       x1th0y0ph0  x1th0y0ph1  x1th0y1ph0  x1th1y0ph0  x2th0y0ph0  \n0        0.114516   -0.000301    0.001183   -0.000270    0.013114  \n1        0.115627   -0.001010    0.000637   -0.001034    0.013370  \n2        0.113799   -0.000109    0.001406   -0.000114    0.012950  \n3        0.118469   -0.000172    0.000856    0.000029    0.014035  \n4        0.114160   -0.000331    0.001328   -0.000301    0.013033  \n...           ...         ...         ...         ...         ...  \n32890    0.106190    0.000652   -0.002526    0.000675    0.011276  \n32891    0.106634    0.001167   -0.002582    0.000320    0.011371  \n32892    0.109042    0.001119   -0.002549    0.000338    0.011890  \n32893    0.106104    0.000632   -0.002379    0.000815    0.011258  \n32894    0.105910    0.000777   -0.002326    0.000789    0.011217  \n\n[32895 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0th0y0ph0</th>\n      <th>x0th0y0ph1</th>\n      <th>x0th0y0ph2</th>\n      <th>x0th0y1ph0</th>\n      <th>x0th0y1ph1</th>\n      <th>x0th0y2ph0</th>\n      <th>x0th1y0ph0</th>\n      <th>x0th1y0ph1</th>\n      <th>x0th1y1ph0</th>\n      <th>x0th2y0ph0</th>\n      <th>x1th0y0ph0</th>\n      <th>x1th0y0ph1</th>\n      <th>x1th0y1ph0</th>\n      <th>x1th1y0ph0</th>\n      <th>x2th0y0ph0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>-0.002624</td>\n      <td>6.886320e-06</td>\n      <td>0.010331</td>\n      <td>-0.000027</td>\n      <td>0.000107</td>\n      <td>-0.002354</td>\n      <td>6.176320e-06</td>\n      <td>-0.000024</td>\n      <td>5.539530e-06</td>\n      <td>0.114516</td>\n      <td>-0.000301</td>\n      <td>0.001183</td>\n      <td>-0.000270</td>\n      <td>0.013114</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-0.008738</td>\n      <td>7.634650e-05</td>\n      <td>0.005508</td>\n      <td>-0.000048</td>\n      <td>0.000030</td>\n      <td>-0.008940</td>\n      <td>7.811040e-05</td>\n      <td>-0.000049</td>\n      <td>7.991500e-05</td>\n      <td>0.115627</td>\n      <td>-0.001010</td>\n      <td>0.000637</td>\n      <td>-0.001034</td>\n      <td>0.013370</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>-0.000955</td>\n      <td>9.125640e-07</td>\n      <td>0.012355</td>\n      <td>-0.000012</td>\n      <td>0.000153</td>\n      <td>-0.001000</td>\n      <td>9.553490e-07</td>\n      <td>-0.000012</td>\n      <td>1.000140e-06</td>\n      <td>0.113799</td>\n      <td>-0.000109</td>\n      <td>0.001406</td>\n      <td>-0.000114</td>\n      <td>0.012950</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>-0.001454</td>\n      <td>2.114490e-06</td>\n      <td>0.007224</td>\n      <td>-0.000011</td>\n      <td>0.000052</td>\n      <td>0.000248</td>\n      <td>-3.604800e-07</td>\n      <td>0.000002</td>\n      <td>6.145490e-08</td>\n      <td>0.118469</td>\n      <td>-0.000172</td>\n      <td>0.000856</td>\n      <td>0.000029</td>\n      <td>0.014035</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>-0.002899</td>\n      <td>8.404720e-06</td>\n      <td>0.011637</td>\n      <td>-0.000034</td>\n      <td>0.000135</td>\n      <td>-0.002637</td>\n      <td>7.645830e-06</td>\n      <td>-0.000031</td>\n      <td>6.955460e-06</td>\n      <td>0.114160</td>\n      <td>-0.000331</td>\n      <td>0.001328</td>\n      <td>-0.000301</td>\n      <td>0.013033</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>32890</th>\n      <td>1</td>\n      <td>0.006142</td>\n      <td>3.771990e-05</td>\n      <td>-0.023787</td>\n      <td>-0.000146</td>\n      <td>0.000566</td>\n      <td>0.006353</td>\n      <td>3.901530e-05</td>\n      <td>-0.000151</td>\n      <td>4.035530e-05</td>\n      <td>0.106190</td>\n      <td>0.000652</td>\n      <td>-0.002526</td>\n      <td>0.000675</td>\n      <td>0.011276</td>\n    </tr>\n    <tr>\n      <th>32891</th>\n      <td>1</td>\n      <td>0.010942</td>\n      <td>1.197270e-04</td>\n      <td>-0.024216</td>\n      <td>-0.000265</td>\n      <td>0.000586</td>\n      <td>0.003000</td>\n      <td>3.282320e-05</td>\n      <td>-0.000073</td>\n      <td>8.998440e-06</td>\n      <td>0.106634</td>\n      <td>0.001167</td>\n      <td>-0.002582</td>\n      <td>0.000320</td>\n      <td>0.011371</td>\n    </tr>\n    <tr>\n      <th>32892</th>\n      <td>1</td>\n      <td>0.010258</td>\n      <td>1.052250e-04</td>\n      <td>-0.023374</td>\n      <td>-0.000240</td>\n      <td>0.000546</td>\n      <td>0.003103</td>\n      <td>3.183400e-05</td>\n      <td>-0.000073</td>\n      <td>9.630840e-06</td>\n      <td>0.109042</td>\n      <td>0.001119</td>\n      <td>-0.002549</td>\n      <td>0.000338</td>\n      <td>0.011890</td>\n    </tr>\n    <tr>\n      <th>32893</th>\n      <td>1</td>\n      <td>0.005959</td>\n      <td>3.551330e-05</td>\n      <td>-0.022421</td>\n      <td>-0.000134</td>\n      <td>0.000503</td>\n      <td>0.007686</td>\n      <td>4.580190e-05</td>\n      <td>-0.000172</td>\n      <td>5.907120e-05</td>\n      <td>0.106104</td>\n      <td>0.000632</td>\n      <td>-0.002379</td>\n      <td>0.000815</td>\n      <td>0.011258</td>\n    </tr>\n    <tr>\n      <th>32894</th>\n      <td>1</td>\n      <td>0.007339</td>\n      <td>5.386610e-05</td>\n      <td>-0.021958</td>\n      <td>-0.000161</td>\n      <td>0.000482</td>\n      <td>0.007450</td>\n      <td>5.467950e-05</td>\n      <td>-0.000164</td>\n      <td>5.550520e-05</td>\n      <td>0.105910</td>\n      <td>0.000777</td>\n      <td>-0.002326</td>\n      <td>0.000789</td>\n      <td>0.011217</td>\n    </tr>\n  </tbody>\n</table>\n<p>32895 rows Ã— 15 columns</p>\n</div>"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldatadf = data[data.runID.eq(2257)]\n",
    "# get the theta and phi dimension and get the predictions\n",
    "alldata = alldatadf.drop(labels=['evtID','runID','SieveRowID','SieveColID','CutID','bpmX','bpmY','targCalTh','targCalPh'], axis=1)\n",
    "\n",
    "alldatatensor = torch.tensor(alldata.to_numpy()).float()\n",
    "\n",
    "theta = predictor(theta_model_PATH,alldatatensor)\n",
    "phi   = predictor(phi_model_PATH,alldatatensor)\n",
    "\n",
    "alldata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-206-0d6cd9716b20>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alldatadf['predicPh'] =phi.numpy()\n",
      "<ipython-input-206-0d6cd9716b20>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alldatadf['predicTh'] =theta.numpy()\n",
      "<ipython-input-206-0d6cd9716b20>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alldatadf['residuTh'] = alldatadf['predicTh'] -alldatadf['targCalTh']\n",
      "<ipython-input-206-0d6cd9716b20>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  alldatadf['residuPh'] = alldatadf['predicPh'] -alldatadf['targCalPh']\n"
     ]
    },
    {
     "data": {
      "text/plain": "   evtID  runID  CutID  SieveRowID  SieveColID    bpmX      bpmY  x0th0y0ph0  \\\n0      0   2257     31           3           4 -0.0002 -0.000498           1   \n1      1   2257     32           4           4 -0.0002 -0.000498           1   \n2      2   2257     31           3           4 -0.0002 -0.000498           1   \n3      3   2257     38           3           5 -0.0002 -0.000498           1   \n4      4   2257     31           3           4 -0.0002 -0.000498           1   \n\n   x0th0y0ph1    x0th0y0ph2  ...  x1th0y0ph1  x1th0y1ph0  x1th1y0ph0  \\\n0   -0.002624  6.886320e-06  ...   -0.000301    0.001183   -0.000270   \n1   -0.008738  7.634650e-05  ...   -0.001010    0.000637   -0.001034   \n2   -0.000955  9.125640e-07  ...   -0.000109    0.001406   -0.000114   \n3   -0.001454  2.114490e-06  ...   -0.000172    0.000856    0.000029   \n4   -0.002899  8.404720e-06  ...   -0.000331    0.001328   -0.000301   \n\n   x2th0y0ph0  targCalTh  targCalPh  predicPh  predicTh  residuTh  residuPh  \n0    0.013114   0.005979  -0.008499 -0.008385  0.006534  0.000555  0.000114  \n1    0.013370   0.019564  -0.008499 -0.008391  0.023369  0.003805  0.000108  \n2    0.012950   0.005979  -0.008499 -0.008798  0.002628 -0.003351 -0.000299  \n3    0.014035  -0.000813  -0.006047 -0.006032 -0.001693 -0.000880  0.000015  \n4    0.013033   0.005979  -0.008499 -0.009247  0.007642  0.001663 -0.000748  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>evtID</th>\n      <th>runID</th>\n      <th>CutID</th>\n      <th>SieveRowID</th>\n      <th>SieveColID</th>\n      <th>bpmX</th>\n      <th>bpmY</th>\n      <th>x0th0y0ph0</th>\n      <th>x0th0y0ph1</th>\n      <th>x0th0y0ph2</th>\n      <th>...</th>\n      <th>x1th0y0ph1</th>\n      <th>x1th0y1ph0</th>\n      <th>x1th1y0ph0</th>\n      <th>x2th0y0ph0</th>\n      <th>targCalTh</th>\n      <th>targCalPh</th>\n      <th>predicPh</th>\n      <th>predicTh</th>\n      <th>residuTh</th>\n      <th>residuPh</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2257</td>\n      <td>31</td>\n      <td>3</td>\n      <td>4</td>\n      <td>-0.0002</td>\n      <td>-0.000498</td>\n      <td>1</td>\n      <td>-0.002624</td>\n      <td>6.886320e-06</td>\n      <td>...</td>\n      <td>-0.000301</td>\n      <td>0.001183</td>\n      <td>-0.000270</td>\n      <td>0.013114</td>\n      <td>0.005979</td>\n      <td>-0.008499</td>\n      <td>-0.008385</td>\n      <td>0.006534</td>\n      <td>0.000555</td>\n      <td>0.000114</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2257</td>\n      <td>32</td>\n      <td>4</td>\n      <td>4</td>\n      <td>-0.0002</td>\n      <td>-0.000498</td>\n      <td>1</td>\n      <td>-0.008738</td>\n      <td>7.634650e-05</td>\n      <td>...</td>\n      <td>-0.001010</td>\n      <td>0.000637</td>\n      <td>-0.001034</td>\n      <td>0.013370</td>\n      <td>0.019564</td>\n      <td>-0.008499</td>\n      <td>-0.008391</td>\n      <td>0.023369</td>\n      <td>0.003805</td>\n      <td>0.000108</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2257</td>\n      <td>31</td>\n      <td>3</td>\n      <td>4</td>\n      <td>-0.0002</td>\n      <td>-0.000498</td>\n      <td>1</td>\n      <td>-0.000955</td>\n      <td>9.125640e-07</td>\n      <td>...</td>\n      <td>-0.000109</td>\n      <td>0.001406</td>\n      <td>-0.000114</td>\n      <td>0.012950</td>\n      <td>0.005979</td>\n      <td>-0.008499</td>\n      <td>-0.008798</td>\n      <td>0.002628</td>\n      <td>-0.003351</td>\n      <td>-0.000299</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2257</td>\n      <td>38</td>\n      <td>3</td>\n      <td>5</td>\n      <td>-0.0002</td>\n      <td>-0.000498</td>\n      <td>1</td>\n      <td>-0.001454</td>\n      <td>2.114490e-06</td>\n      <td>...</td>\n      <td>-0.000172</td>\n      <td>0.000856</td>\n      <td>0.000029</td>\n      <td>0.014035</td>\n      <td>-0.000813</td>\n      <td>-0.006047</td>\n      <td>-0.006032</td>\n      <td>-0.001693</td>\n      <td>-0.000880</td>\n      <td>0.000015</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2257</td>\n      <td>31</td>\n      <td>3</td>\n      <td>4</td>\n      <td>-0.0002</td>\n      <td>-0.000498</td>\n      <td>1</td>\n      <td>-0.002899</td>\n      <td>8.404720e-06</td>\n      <td>...</td>\n      <td>-0.000331</td>\n      <td>0.001328</td>\n      <td>-0.000301</td>\n      <td>0.013033</td>\n      <td>0.005979</td>\n      <td>-0.008499</td>\n      <td>-0.009247</td>\n      <td>0.007642</td>\n      <td>0.001663</td>\n      <td>-0.000748</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 28 columns</p>\n</div>"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldatadf['predicPh'] =phi.numpy()\n",
    "alldatadf['predicTh'] =theta.numpy()\n",
    "alldatadf['residuTh'] = alldatadf['predicTh'] -alldatadf['targCalTh']\n",
    "alldatadf['residuPh'] = alldatadf['predicPh'] -alldatadf['targCalPh']\n",
    "alldatadf.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist2d(alldatadf['predicPh'],alldatadf['predicTh'],bins=300,cmap=plt.cm.BuPu)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}